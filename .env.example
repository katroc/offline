### MCP Server
MCP_PORT=8787
MCP_HOST=127.0.0.1

### Document Sources Configuration
# Confluence (required for real RAG - leave empty to use mock data)
CONFLUENCE_BASE_URL=https://confluence.local
CONFLUENCE_USERNAME=service-account
CONFLUENCE_API_TOKEN=your-api-token-here

### LLM Configuration (Ollama or OpenAI-compatible)
# Set this to enable LLM answers; leave empty to use stubbed answers
LLM_BASE_URL=http://127.0.0.1:11434
LLM_CHAT_MODEL=openai/gpt-oss-20b
# Default to Gemma embedding model (available in your setup)
LLM_EMBED_MODEL=text-embedding-embeddinggemma-300m-qat

### Request timeouts
REQUEST_TIMEOUT_MS=15000

### Vector Database
LANCEDB_PATH=./data/lancedb
CHUNK_TTL_DAYS=7
MIN_VECTOR_RESULTS=3
ADAPTIVE_THRESHOLD=false
# Optional lexical floor for vector results (0..1)
MIN_KEYWORD_SCORE=0.0

### Logging
LOG_LEVEL=info

### Admin
# Protect admin endpoints for sync/reindex
ADMIN_API_KEY=change-me

### Crawler / Ingestion (background)
# Comma-separated list of Confluence spaces to crawl (e.g., ENG,OPS,HR)
CRAWL_SPACES=
# Cron for periodic crawl (use standard 5-field cron or leave empty to disable)
# Example: every 15 minutes: */15 * * * *
CRAWL_CRON=
# Max pages per tick per space (pagination)
CRAWL_PAGE_SIZE=50
CRAWL_MAX_PAGES_PER_TICK=200
# Concurrency of page-level indexing (per process)
CRAWL_CONCURRENCY=4

# Rate limit and retries
# Confluence calls: minimum interval between requests (ms). 0 to disable pacing.
CONFLUENCE_MIN_INTERVAL_MS=150
# Embedding: batch size per request and optional delay between batches (ms)
EMBED_BATCH_SIZE=16
EMBED_DELAY_MS=0
# Embedding retry policy (for 429/5xx)
EMBED_MAX_RETRIES=3
EMBED_BACKOFF_BASE_MS=250
# LanceDB schema behavior
# If true, throw on schema mismatch; if false, silently drop unknown fields (e.g., url) and continue
LANCEDB_STRICT_SCHEMA=false
